{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himesh-Sarkar/Text-prediction-noob/blob/main/text_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODUJDkjBlFSN"
      },
      "outputs": [],
      "source": [
        "faqs=\"\"\"\n",
        "Exploring Long Short-Term Memory (LSTM) for Text Prediction: Techniques, Applications, and Challenges.\n",
        "Abstract: In the realm of natural language processing (NLP), text prediction stands as a fundamental task with applications ranging from language modeling to autocomplete systems and predictive typing.\n",
        "Among the various techniques employed for text prediction, Long Short-Term Memory (LSTM) networks have emerged as a powerful tool, capable of capturing long-range dependencies and preserving context over extended sequences.\n",
        "This essay delves into the principles, methodologies, applications, and challenges associated with LSTM-based text prediction, exploring its significance in modern NLP research and applications.\n",
        "Introduction: Text prediction, also known as language modeling or next-word prediction, refers to the task of predicting the next word or sequence of words in a given text based on the preceding context.\n",
        "This task plays a pivotal role in various NLP applications, including machine translation, speech recognition, sentiment analysis, and autocomplete systems.\n",
        "Traditional methods for text prediction often rely on statistical language models or n-gram models, which suffer from limited contextual understanding and struggle to capture long-range dependencies within the text.\n",
        "In recent years, deep learning approaches, particularly recurrent neural networks (RNNs) and their variants, have demonstrated remarkable performance in text prediction tasks.\n",
        "Among these variants, LSTM networks have garnered significant attention due to their ability to mitigate the vanishing gradient problem and capture long-term dependencies in sequential data.\n",
        "LSTM networks, with their gated architecture, offer an effective solution for modeling sequential data with complex temporal dynamics, making them well-suited for text prediction tasks.\n",
        "Principles of LSTM: LSTM networks are a type of recurrent neural network architecture designed to overcome the limitations of traditional RNNs in capturing long-range dependencies.\n",
        "At the core of LSTM networks are memory cells equipped with various gates, including input gates, forget gates, and output gates.\n",
        "These gates regulate the flow of information within the network, allowing it to retain relevant information over long sequences while filtering out irrelevant information.\n",
        "The key components of an LSTM cell include: Forget Gate: Determines which information from the previous cell state should be discarded or forgotten.\n",
        "Input Gate: Determines which new information from the current input should be stored in the cell state.\n",
        "Cell State: Represents the long-term memory of the network, preserving information across time steps.\n",
        "Output Gate: Determines which information from the current cell state should be output to the next time step.\n",
        "By learning to selectively update and propagate information through the gates, LSTM networks can effectively capture long-term dependencies and retain contextual information over extended sequences, making them well-suited for text prediction tasks.\n",
        "Methodologies for LSTM-based Text Prediction: LSTM-based text prediction involves several key methodologies and techniques aimed at training, evaluating, and deploying predictive models.\n",
        "These methodologies include: Data Preparation: Preprocessing and tokenization of text data, including text cleaning, tokenization, and sequence padding.\n",
        "Model Architecture: Designing the architecture of the LSTM network, including the number of layers, hidden units, and embedding dimensions.\n",
        "Training Procedure: Training the LSTM model using backpropagation through time (BPTT) or truncated BPTT with optimization techniques such as stochastic gradient descent (SGD) or Adam.\n",
        "Hyperparameter Tuning: Optimizing hyperparameters such as learning rate, dropout rate, and batch size to improve model performance.\n",
        "Evaluation Metrics: Assessing model performance using evaluation metrics such as perplexity, cross-entropy loss, and accuracy on held-out validation or test sets.\n",
        "Fine-Tuning and Transfer Learning: Fine-tuning pre-trained LSTM models on domain-specific data or using transfer learning techniques to leverage pre-trained language models for text prediction tasks.\n",
        "Applications of LSTM-based Text Prediction: LSTM-based text prediction finds applications across various domains and industries, including: Autocomplete Systems: Providing real-time suggestions for the next word or phrase in text input fields, enhancing user experience in search engines, messaging apps, and code editors.\n",
        "Speech Recognition: Predicting the next word or sentence in speech recognition systems, improving transcription accuracy and naturalness in speech-to-text applications.\n",
        "Language Modeling: Generating coherent and contextually relevant text based on a given prompt or seed text, enabling applications such as chatbots, virtual assistants, and content generation.\n",
        "Machine Translation: Predicting the next word or phrase in machine translation systems, facilitating the translation of text between different languages with improved fluency and coherence.\n",
        "Sentiment Analysis: Predicting the sentiment or emotion of text inputs, aiding in sentiment analysis tasks such as opinion mining, social media monitoring, and customer feedback analysis.\n",
        "Challenges and Future Directions: Despite the significant progress and success achieved with LSTM-based text prediction, several challenges and areas for improvement remain: Handling Long-Term Dependencies: While LSTM networks excel at capturing long-term dependencies, they may still struggle with extremely long sequences or contexts, leading to information loss or forgetting.\n",
        "Overfitting and Generalization: LSTM models are prone to overfitting, especially when trained on limited data or noisy text corpora.\n",
        "Techniques such as regularization, dropout, and data augmentation can help alleviate overfitting and improve generalization.\n",
        "Computational Efficiency: Training and deploying large-scale LSTM models can be computationally intensive, requiring significant computational resources and memory.\n",
        "Efficient model architectures and optimization techniques are needed to address these challenges.\n",
        "Interpretability and Explainability: LSTM models are often viewed as black-box models, making it challenging to interpret their internal workings and decision-making processes.\n",
        "Methods for interpreting and explaining model predictions are essential for building trust and understanding in LSTM-based text prediction systems.\n",
        "Conclusion: In conclusion, LSTM-based text prediction represents a powerful approach for modeling and generating text with rich contextual understanding and long-term dependencies.\n",
        "By leveraging the principles of gated recurrent networks, LSTM models have demonstrated superior performance in various text prediction tasks across different domains and applications.\n",
        "However, addressing challenges such as handling long-term dependencies, overfitting, and computational efficiency remains crucial for advancing the state-of-the-art in LSTM-based text prediction.\n",
        "With ongoing research and innovation, LSTM networks are poised to play an increasingly significant role in shaping the future of natural language processing and intelligent text-based applications.\n",
        "References:\n",
        "Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n",
        "Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.\n",
        "Karpathy, A., & Fei-Fei, L. (2015). Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078.\n",
        "Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n",
        "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCLkLuKOmIba"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO9wWZtPnMM-"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([faqs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GORfaWTnMEW",
        "outputId": "9fe876d2-c992-440c-ab5b-2d7f419d1326"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "431"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ndadC3ruHuX",
        "outputId": "c718116d-304d-48c9-d024-c970a07c07e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index['model']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LurrqU1bszm1",
        "outputId": "0de48546-543a-4e30-a4af-954c74893576"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[97, 10],\n",
              " [97, 10, 70],\n",
              " [97, 10, 70, 19],\n",
              " [97, 10, 70, 19, 28],\n",
              " [97, 10, 70, 19, 28, 4],\n",
              " [97, 10, 70, 19, 28, 4, 11]]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequence=[]\n",
        "for sentence in faqs.split('\\n'):\n",
        "  tokenized_sentence=tokenizer.texts_to_sequences([sentence])[0]\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequence.append(tokenized_sentence[:i+1])\n",
        "\n",
        "input_sequence[:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvIFGHz7xeWv",
        "outputId": "23c7461f-6269-48ea-c629-3f2fc15a7984"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len=max([len(x) for x in input_sequence])\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN-GUtqKr1R-",
        "outputId": "7caf58bd-063c-4d69-b519-5a04d52546ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  97,  10],\n",
              "       [  0,   0,   0, ...,  97,  10,  70],\n",
              "       [  0,   0,   0, ...,  10,  70,  19],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  18,  71,  24],\n",
              "       [  0,   0,   0, ...,  71,  24, 430],\n",
              "       [  0,   0,   0, ...,  24, 430, 431]], dtype=int32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences=pad_sequences(input_sequence,maxlen=max_len,padding='pre')\n",
        "padded_input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUpmXKVzy8sk"
      },
      "outputs": [],
      "source": [
        "X=padded_input_sequences[:,:-1]\n",
        "y=padded_input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahQr7olYzUgU",
        "outputId": "97222c4e-610e-4770-9855-098ae8e1ee23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1051, 51), (1051,))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape,y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxzcACDezVpU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y=to_categorical(y,num_classes=432)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFl9eDBC1tEt",
        "outputId": "1da27347-6080-4b9d-8a44-73fdc56c9999"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1051, 432)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGNGv-HP13wM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RXInWxS3Xr9"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(432, 100, input_length=51))\n",
        "model.add(LSTM(200))\n",
        "model.add(Dense(432,activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIzx2LFy5fib"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2TZ46EI5lad",
        "outputId": "2b3c092d-029c-41ee-d17e-a4027b885161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 51, 100)           43200     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 200)               240800    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 432)               86832     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 370832 (1.41 MB)\n",
            "Trainable params: 370832 (1.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHxW1zlr5mCu",
        "outputId": "b0e3ef35-4a8b-42c3-e9c4-e95549f85380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 8s 133ms/step - loss: 5.9009 - accuracy: 0.0314\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 4s 129ms/step - loss: 5.5594 - accuracy: 0.0390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 6s 169ms/step - loss: 5.4759 - accuracy: 0.0457\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 4s 133ms/step - loss: 5.4279 - accuracy: 0.0466\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 5s 146ms/step - loss: 5.3536 - accuracy: 0.0590\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 5s 151ms/step - loss: 5.2163 - accuracy: 0.0866\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 5.0116 - accuracy: 0.0894\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 6s 171ms/step - loss: 4.7243 - accuracy: 0.1189\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 4.4027 - accuracy: 0.1408\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 4.0569 - accuracy: 0.1760\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 6s 172ms/step - loss: 3.6977 - accuracy: 0.2388\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 4s 131ms/step - loss: 3.3702 - accuracy: 0.2788\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 5s 143ms/step - loss: 3.0485 - accuracy: 0.3292\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 5s 154ms/step - loss: 2.7460 - accuracy: 0.3749\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 4s 129ms/step - loss: 2.4808 - accuracy: 0.4405\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 6s 168ms/step - loss: 2.2162 - accuracy: 0.5243\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 5s 159ms/step - loss: 1.9895 - accuracy: 0.5861\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 5s 150ms/step - loss: 1.7663 - accuracy: 0.6384\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 5s 159ms/step - loss: 1.5893 - accuracy: 0.6993\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 4s 129ms/step - loss: 1.4081 - accuracy: 0.7650\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 6s 171ms/step - loss: 1.2572 - accuracy: 0.8069\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 4s 131ms/step - loss: 1.1254 - accuracy: 0.8354\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 4s 131ms/step - loss: 1.0016 - accuracy: 0.8630\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.8957 - accuracy: 0.8849\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 0.8035 - accuracy: 0.9077\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 4s 135ms/step - loss: 0.7217 - accuracy: 0.9343\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 5s 163ms/step - loss: 0.6429 - accuracy: 0.9477\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 0.5797 - accuracy: 0.9524\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 5s 159ms/step - loss: 0.5236 - accuracy: 0.9629\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 4s 133ms/step - loss: 0.4751 - accuracy: 0.9657\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 4s 128ms/step - loss: 0.4301 - accuracy: 0.9724\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 5s 165ms/step - loss: 0.3876 - accuracy: 0.9781\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 4s 126ms/step - loss: 0.3518 - accuracy: 0.9810\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 4s 127ms/step - loss: 0.3204 - accuracy: 0.9810\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.2959 - accuracy: 0.9838\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 4s 127ms/step - loss: 0.2705 - accuracy: 0.9867\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 5s 144ms/step - loss: 0.2468 - accuracy: 0.9838\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 5s 151ms/step - loss: 0.2264 - accuracy: 0.9886\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 0.2108 - accuracy: 0.9886\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 5s 166ms/step - loss: 0.1953 - accuracy: 0.9895\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 4s 130ms/step - loss: 0.1826 - accuracy: 0.9914\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 4s 130ms/step - loss: 0.1707 - accuracy: 0.9886\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 6s 172ms/step - loss: 0.1608 - accuracy: 0.9876\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 4s 124ms/step - loss: 0.1471 - accuracy: 0.9914\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 4s 130ms/step - loss: 0.1379 - accuracy: 0.9914\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 6s 170ms/step - loss: 0.1298 - accuracy: 0.9933\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 4s 128ms/step - loss: 0.1225 - accuracy: 0.9914\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 6s 172ms/step - loss: 0.1157 - accuracy: 0.9886\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 0.1097 - accuracy: 0.9905\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 4s 130ms/step - loss: 0.1026 - accuracy: 0.9943\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 5s 162ms/step - loss: 0.0976 - accuracy: 0.9933\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 4s 127ms/step - loss: 0.0926 - accuracy: 0.9914\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.0867 - accuracy: 0.9933\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 5s 133ms/step - loss: 0.0828 - accuracy: 0.9914\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 4s 131ms/step - loss: 0.0781 - accuracy: 0.9943\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 6s 169ms/step - loss: 0.0753 - accuracy: 0.9943\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 4s 130ms/step - loss: 0.0721 - accuracy: 0.9933\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 0.0687 - accuracy: 0.9943\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 6s 171ms/step - loss: 0.0656 - accuracy: 0.9952\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 4s 129ms/step - loss: 0.0637 - accuracy: 0.9943\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 5s 140ms/step - loss: 0.0602 - accuracy: 0.9943\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 5s 150ms/step - loss: 0.0580 - accuracy: 0.9943\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 4s 128ms/step - loss: 0.0561 - accuracy: 0.9962\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 5s 166ms/step - loss: 0.0536 - accuracy: 0.9943\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 0.0516 - accuracy: 0.9952\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 4s 131ms/step - loss: 0.0496 - accuracy: 0.9943\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 5s 166ms/step - loss: 0.0471 - accuracy: 0.9943\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 4s 128ms/step - loss: 0.0458 - accuracy: 0.9943\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 4s 127ms/step - loss: 0.0439 - accuracy: 0.9943\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 6s 170ms/step - loss: 0.0435 - accuracy: 0.9924\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 0.0410 - accuracy: 0.9952\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 5s 153ms/step - loss: 0.0410 - accuracy: 0.9933\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 4s 132ms/step - loss: 0.0392 - accuracy: 0.9952\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 4s 129ms/step - loss: 0.0388 - accuracy: 0.9943\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 5s 159ms/step - loss: 0.0392 - accuracy: 0.9943\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 4s 127ms/step - loss: 0.0369 - accuracy: 0.9943\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 4s 129ms/step - loss: 0.0357 - accuracy: 0.9952\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 5s 158ms/step - loss: 0.0344 - accuracy: 0.9943\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 0.0331 - accuracy: 0.9924\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 5s 144ms/step - loss: 0.0325 - accuracy: 0.9943\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 5s 141ms/step - loss: 0.0304 - accuracy: 0.9962\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 0.0304 - accuracy: 0.9952\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 5s 161ms/step - loss: 0.0300 - accuracy: 0.9933\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 4s 128ms/step - loss: 0.0298 - accuracy: 0.9924\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 0.0288 - accuracy: 0.9943\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 5s 159ms/step - loss: 0.0275 - accuracy: 0.9943\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 4s 129ms/step - loss: 0.0264 - accuracy: 0.9952\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 4s 131ms/step - loss: 0.0268 - accuracy: 0.9943\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 6s 187ms/step - loss: 0.0255 - accuracy: 0.9962\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 4s 128ms/step - loss: 0.0258 - accuracy: 0.9952\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 5s 160ms/step - loss: 0.0241 - accuracy: 0.9962\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 4s 127ms/step - loss: 0.0244 - accuracy: 0.9952\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 4s 124ms/step - loss: 0.0240 - accuracy: 0.9943\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 5s 163ms/step - loss: 0.0231 - accuracy: 0.9952\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 4s 125ms/step - loss: 0.0231 - accuracy: 0.9933\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 4s 136ms/step - loss: 0.0216 - accuracy: 0.9943\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 5s 148ms/step - loss: 0.0216 - accuracy: 0.9943\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 4s 131ms/step - loss: 0.0214 - accuracy: 0.9952\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 5s 160ms/step - loss: 0.0211 - accuracy: 0.9943\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 4s 130ms/step - loss: 0.0206 - accuracy: 0.9952\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cd21da9b790>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X,y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "5uyuPQ_Z_G8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"drawback of lstm \"\n",
        "for i in range(60):\n",
        "  token_text=tokenizer.texts_to_sequences([text])[0]\n",
        "  padded_token_text=pad_sequences([token_text], maxlen=51, padding='pre')\n",
        "  pos=np.argmax(model.predict(padded_token_text))\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text=text+\" \"+word\n",
        "      print(text)"
      ],
      "metadata": {
        "id": "PtLM_gkp9yJH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM/UFlrxdMwjoyfO+DKWeh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}